这篇 Distributed Representations of Words and Phrases and their Compositionality 由 Mikolov 等人在 2013 年 NeurIPS 发表，接在前一篇 Skip-gram 工作之后，核心目标是：在超大规模语料上，高效学习能精确刻画语法和语义关系的词与短语向量。作者强调，Skip-gram 不需要大规模稠密矩阵乘法，在单机优化实现下，一天可以在百亿级词数据上训练完，是当时极少数真正能在工业规模语料上跑起来的神经词向量模型。得到的向量不仅能把相似词聚到一起，还自然呈现出线性结构，例如 “Madrid − Spain + France ≈ Paris”，揭示出某种“向量平移 = 语义关系”的规律。

在这个基础上，论文第一部分系统解决“如何让 Skip-gram 又快又好”的问题。经典 Skip-gram 目标是：给定序列中每个中心词，去预测一定窗口内的上下文词，但直接用完整 softmax 会让复杂度与词表大小线性相关，因此作者先复盘了分层 softmax 的做法，再提出更简单的 负采样（Negative Sampling）：把“真实的中心词–上下文词”视为正例，再从噪声分布中采若干随机词与中心词组成负例，通过一次 logistic 回归把正例打高分、负例打低分即可，不再追求精确重建整棵 softmax。负采样可以看作对 NCE 的“为表示学习量身瘦身”：不再关心严格的概率建模，只要学到好向量即可。配合把高频词出现概率压到 3/4 次方的噪声分布，以及对 “the, of, in” 等高频词的子采样，实验表明在 10 亿词语料上，负采样在类比任务上的准确率明显优于分层 softmax 和原始 NCE，同时训练速度提升 2–10 倍，并显著改善稀有词表示质量

第二个重点是把“词向量”自然扩展到“短语向量”。作者指出，用单词向量很难表示 “Air Canada”“Boston Globe” 这类整体语义远不等于逐词相加的短语，因此他们采用一个完全数据驱动的短语挖掘过程：统计相邻词的共现频率，用简单打分函数挑出“经常一起出现、而分别出现并不极端高频”的词对，将其合并成新的 token，如 “New_York_Times”“Toronto_Maple_Leafs”，并可以迭代几轮形成更长短语。之后在“词 + 短语”统一词表上，用同一个 Skip-gram + 负采样目标训练出统一空间中的向量。为评估短语表示，他们构造了包含 3000 余条的短语类比数据集，比如 “Montreal : Montreal Canadiens :: Toronto : Toronto Maple Leafs”，模型需要通过向量运算找出正确球队。最好的大规模模型在该集合上达到约 72% 的准确率，说明短语向量同样具备良好的结构化语义关系

最后，论文用大量可视化和实验凸显 Skip-gram 表示的“线性结构”和“加法可组合性”。一方面，他们在类比测试集中系统评估语义类比（国家–首都、国家–货币等）和语法类比（单复数、形容词–副词等），在 10 亿词新闻语料上训练的 300 维模型，配合负采样和子采样后，整体类比准确率可达 60% 左右；当扩展到 330 亿词、1000 维向量并使用整句上下文时，短语类比性能进一步提升。另一方面，作者展示了两个重要“玩具”：其一是国家与首都在二维 PCA 平面上几乎形成平行线段；其二是简单向量加法如 “Russia + river ≈ Volga River”“Germany + capital ≈ Berlin”。这些现象表明，Skip-gram 在纯无监督条件下学到的向量已经内含大量语义与句法规则，而这些规则可以通过线性代数这种极简单的操作被显性读出。正是这种“结构强、训练快、易复现”的组合，使得 word2vec 很快成为 NLP 社区的标准词表示方法，并为后续从 GloVe 到 fastText、再到大规模预训练语言模型的嵌入层设计打下了基础
